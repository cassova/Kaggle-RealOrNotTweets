{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Real or Not?  NLP with Disaster Tweets with Deep Learning\n",
    "\n",
    "Some resources I used:\n",
    " - https://www.kaggle.com/philculliton/nlp-getting-started-tutorial\n",
    " - https://www.kaggle.com/abhishek/approaching-almost-any-nlp-problem-on-kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "pd.set_option('display.max_columns', 500)\n",
    "train_df = pd.read_csv('train.csv')\n",
    "test_df = pd.read_csv('test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Data Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train has 7613 records\n",
      "Test has 3263 records\n",
      "\n",
      "Keyword Values: Train = 0.801% Test = 0.797%\n",
      "Location Values: Train = 33.272% Test = 33.865%\n"
     ]
    }
   ],
   "source": [
    "print (f'Train has {len(train_df)} records\\nTest has {len(test_df)} records\\n')\n",
    "\n",
    "null_train_keyword = train_df['keyword'].isnull().sum() / len(train_df) * 100\n",
    "null_test_keyword = test_df['keyword'].isnull().sum() / len(test_df) * 100\n",
    "null_train_location = train_df['location'].isnull().sum() / len(train_df) * 100\n",
    "null_test_location = test_df['location'].isnull().sum() / len(test_df) * 100\n",
    "\n",
    "print (f'Keyword Values: Train = {round(null_train_keyword,3)}% Test = {round(null_test_keyword,3)}%')\n",
    "print (f'Location Values: Train = {round(null_train_location,3)}% Test = {round(null_test_location,3)}%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train contains 61 records with no keywords\n",
      "  21 of which don't have any '#'\n"
     ]
    }
   ],
   "source": [
    "# Let's see if we can use a '#' value for our keyword...\n",
    "null_train_keyword_df = train_df[train_df['keyword'].isnull()]['text'].str.contains('#')\n",
    "\n",
    "print(f'Train contains {len(null_train_keyword_df)} records with no keywords')\n",
    "print(f'  {null_train_keyword_df.sum()} of which don\\'t have any \\'#\\'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    This is ALL CAPS <allcaps> too longggg\n",
       "Name: test, dtype: object"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Testing area:\n",
    "eyes = r\"[8:=;]\"\n",
    "nose = r\"['`\\-]?\"\n",
    "df = pd.DataFrame({'test': ['This is ALL CAPS too longggg']})\n",
    "df['test'].str.replace(r\" ([A-Z -_]{2,}) \", r' \\1 <allcaps> ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "eyes = r\"[8:=;]\"\n",
    "nose = r\"['`\\-]?\"\n",
    "key_words = ['user', 'number', 'hashtag', 'repeat', 'smile', \n",
    "             'lolface', 'sadface', 'neutralface', 'heart',\n",
    "             'elong', 'allcaps', 'url']\n",
    "\n",
    "all_data = [train_df, test_df]\n",
    "\n",
    "for df in all_data:\n",
    "    # Replace websites URLs\n",
    "    df['text'] = df['text'].str.replace('http\\S+|www.\\S+', '<url>', case=False)\n",
    "    # Replace usernames\n",
    "    df['text'] = df['text'].str.replace('@\\S+', ' <user>')\n",
    "    # Remove encodings like &amp; and &gt;\n",
    "    df['text'] = df['text'].str.replace('&\\S+;', '') # not used in GloVe\n",
    "    # Replace numbers\n",
    "    df['text'] = df['text'].str.replace(r\"[-+]?[.\\d]*[\\d]+[:,.\\d]*\", \"<number>\")\n",
    "    # Replace hashtags\n",
    "    df['text'] = df['text'].str.replace('#', '<hashtag> ')\n",
    "    # Replace repeat !! ?? (not words)\n",
    "    #df['text'] = df['text'].str.replace(r'(?<!\\S)((\\S+))(?:\\s+\\2)+(?!\\S)', r'\\1 <repeat>') # words: my misunderstanding\n",
    "    df['text'] = df['text'].str.replace(r\"([!?.]){2,}\", r\"\\1 <repeat>\")\n",
    "    # Replace emoticons\n",
    "    df['text'] = df['text'].str.replace(r\"{}{}[)dD]+|[)dD]+{}{}\".format(eyes, nose, nose, eyes), \"<smile>\")\n",
    "    df['text'] = df['text'].str.replace(r\"{}{}p+\".format(eyes, nose), \"<lolface>\")\n",
    "    df['text'] = df['text'].str.replace(r\"{}{}\\(+|\\)+{}{}\".format(eyes, nose, nose, eyes), \"<sadface>\")\n",
    "    df['text'] = df['text'].str.replace(r\"{}{}[\\/|l*]\".format(eyes, nose), \"<neutralface>\")\n",
    "    df['text'] = df['text'].str.replace(r\"<3\",\"<heart>\")\n",
    "    # Elongated words like wayyyyy too longgg\n",
    "    df['text'] = df['text'].str.replace(r\"\\b(\\S*?)(.)\\2{2,}\\b\", r\"\\1\\2 <elong>\")\n",
    "    # ALL CAPS\n",
    "    df['text'] = df['text'].str.replace(r\" ([A-Z -_]{2,}) \", r' \\1 <allcaps> ')\n",
    "    # Remove *\n",
    "    df['text'] = df['text'].str.replace(r\"\\*\", r'')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train = 5077 records\n",
      "Validate = 2536 records\n"
     ]
    }
   ],
   "source": [
    "xtrain_full = train_df['text']\n",
    "xtest_full = test_df['text']\n",
    "ytrain_full = train_df['target']\n",
    "xtrain, xvalid, ytrain, yvalid = train_test_split(train_df.text.values, ytrain_full, \n",
    "                                                  stratify=ytrain_full, \n",
    "                                                  random_state=42, \n",
    "                                                  test_size=0.333, shuffle=True)\n",
    "\n",
    "print(f'Train = {len(xtrain)} records\\nValidate = {len(xvalid)} records')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Global Vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1e9ef12881e341a5bb7ab3ec53603067",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=1.0, bar_style='info', max=1.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Found 1193514 word vectors.\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm  # This is an awesome library that shows the progress of whatever tqdm() is applied to\n",
    "\n",
    "#with open('D:\\Datasets\\GloVe\\glove.twitter.27B.25d.txt', 'r', encoding=\"utf8\") as f:\n",
    "with open('D:\\Datasets\\GloVe\\glove.twitter.27B.200d.txt', 'r', encoding=\"utf8\") as f:\n",
    "    embeddings_index = {}\n",
    "    for line in tqdm(f):\n",
    "        vals = line.rstrip().split(' ')\n",
    "        embeddings_index[vals[0]] = [float(x) for x in vals[1:]]\n",
    "print('Found %s word vectors.' % len(embeddings_index))  #1193514 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "screams internally\n",
      "['screams', 'internally']\n",
      "['screams', 'internally']\n",
      "[ 6.70529390e-02  4.43065556e-02  1.49141888e-02  7.36671104e-02\n",
      " -1.34542076e-02  1.14275995e-01  1.26520787e-01  1.63734215e-01\n",
      "  5.48288245e-04 -8.43475034e-02 -1.92787878e-02 -2.65030936e-02\n",
      " -1.26914694e-01 -4.21059174e-02  8.09426895e-03 -1.06641128e-01\n",
      "  4.30478317e-02 -5.17430657e-02 -3.90803207e-02 -1.33606430e-02\n",
      "  4.97997301e-02  3.16750330e-02  3.06803763e-02 -2.51013095e-02\n",
      "  1.32639909e-02  3.05011067e-01 -2.26566550e-03 -1.80953834e-02\n",
      " -4.70629671e-02  2.87402219e-02 -8.27007674e-02  6.32664753e-02\n",
      "  3.09941918e-02 -1.90176491e-02 -1.21079073e-01 -6.06447967e-02\n",
      " -3.78739929e-02 -2.74410781e-02  1.02910710e-01  4.38003714e-02\n",
      "  1.67318672e-01 -5.89269517e-02 -2.62017222e-02  1.21699406e-01\n",
      " -1.46393897e-01  6.78174549e-02  7.50682395e-02  4.48230318e-02\n",
      " -6.71260129e-02  1.42124547e-02 -8.80199119e-02  7.07095350e-02\n",
      "  1.15412804e-02  1.74016958e-02  2.89707930e-02 -1.28155360e-02\n",
      " -1.01446424e-01  8.62075666e-02  5.54164099e-02  6.70174780e-02\n",
      "  2.72216693e-02 -6.37558179e-02  7.49648506e-02 -5.51787559e-02\n",
      " -4.70788731e-02 -2.73114912e-02 -2.95476839e-02  1.79194820e-02\n",
      "  2.92123486e-03  1.41502343e-01  1.73452764e-02  8.73518610e-03\n",
      " -3.09218664e-02 -7.17930125e-02  4.18079143e-02  1.05770042e-01\n",
      "  1.14547332e-02 -9.12553739e-03 -1.03898751e-01  5.67160214e-02\n",
      " -1.06851648e-01  4.08175336e-02 -4.12151829e-03 -5.85143320e-02\n",
      " -7.82775970e-02 -1.58583486e-01 -4.46031552e-02  4.98568045e-02\n",
      " -9.11646163e-02  8.25763265e-02 -1.87040208e-02  3.24378928e-02\n",
      "  4.92738973e-02  2.61308938e-02  7.10061347e-03 -8.84418880e-03\n",
      " -8.81593230e-02 -1.49831459e-01  4.51944831e-02 -3.51980469e-02\n",
      " -4.06034111e-02  4.25550272e-02 -3.23518134e-02  1.86829687e-02\n",
      "  1.02453179e-03 -4.35084500e-02 -5.51142899e-02  6.50133254e-02\n",
      " -1.75505570e-02 -2.82332892e-02  1.00180496e-01 -4.68327983e-02\n",
      "  2.71561741e-02 -6.59920106e-02  7.70850233e-02  3.23845610e-02\n",
      " -6.74949379e-02 -1.20249156e-02  6.57655844e-02  7.66431179e-03\n",
      "  9.26541639e-02  3.43844096e-02 -1.80050000e-02 -2.42969351e-02\n",
      " -4.32119439e-02  1.01570865e-01 -5.99729097e-02  1.04782936e-02\n",
      "  2.02710398e-01 -1.11711391e-01 -1.05464086e-01  1.94254034e-02\n",
      " -6.77266973e-02  1.05529581e-01 -1.04841882e-02  1.75379258e-02\n",
      "  4.57717764e-03  2.52426855e-02  1.47644855e-03  3.14844700e-02\n",
      " -4.49596361e-02  1.65549245e-02 -1.33758941e-01 -4.27500438e-02\n",
      "  5.02815875e-03 -7.57348869e-02 -5.22127598e-02 -1.06630836e-01\n",
      " -1.48149168e-02 -1.67265341e-02 -6.28697617e-02  7.77081632e-02\n",
      " -2.04229886e-01 -1.39757364e-01  5.80801925e-03 -4.63537478e-02\n",
      "  2.73194442e-02  7.45709439e-02  1.32630552e-01  5.62423041e-02\n",
      " -6.41076206e-03 -7.66789531e-02  3.21486848e-02 -1.20965860e-01\n",
      "  4.24806434e-02 -4.27767752e-02 -3.80152754e-03 -5.03049787e-02\n",
      " -3.49014473e-02 -3.50799684e-02  6.72832013e-02 -2.11053548e-02\n",
      "  4.58887320e-02 -7.04943366e-02  1.65853452e-01  2.13056765e-02\n",
      " -1.22105476e-02 -8.67249785e-03 -7.91584136e-02  1.78660567e-02\n",
      "  3.03693678e-02  1.02433530e-01  6.92243195e-02  3.42857926e-02\n",
      " -2.83547359e-02 -7.31939545e-02  3.90706835e-02 -2.09453595e-02\n",
      " -1.31224277e-02 -5.54416723e-02 -3.67758259e-03  8.24135242e-02\n",
      " -3.64069009e-02 -5.95267004e-02  1.64673603e-04  2.51293789e-02\n",
      "  1.00709136e-02  1.08393592e-01 -9.34213932e-02  1.04140148e-02]\n"
     ]
    }
   ],
   "source": [
    "# import nltk\n",
    "# nltk.download('punkt')\n",
    "# nltk.download('stopwords')\n",
    "\n",
    "from nltk import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "stop_words = stopwords.words('english')\n",
    "import re\n",
    "\n",
    "def new_tokenize(s):\n",
    "    # This will regroup key_words like '<', 'hashtag', '>' into '<hashtag>'\n",
    "    words = word_tokenize(s)\n",
    "    new_words = []\n",
    "    skip = 0\n",
    "    for i, w in enumerate(words):\n",
    "        if skip > 0:\n",
    "            skip = skip-1\n",
    "        else:\n",
    "            if w == '<' and words[i+1] in key_words and words[i+2] == '>':\n",
    "                new_words.append('<' + words[i+1] + '>')\n",
    "                skip = 2\n",
    "            else:\n",
    "                new_words.append(w)\n",
    "    return new_words\n",
    "            \n",
    "\n",
    "def sent2vec(s):\n",
    "    words = str(s).lower()\n",
    "    words = new_tokenize(words)\n",
    "    words = [w for w in words if not w in stop_words]\n",
    "    words = [w for w in words if w.isalpha() or re.match(\"<\\S+>\",w)]\n",
    "    M = []\n",
    "    for w in words:\n",
    "        try:\n",
    "            # This adds the np.array (size=25) of values from the GloVe file for each word to the matrix\n",
    "            M.append(embeddings_index[w])\n",
    "        except:\n",
    "            continue\n",
    "    M = np.array(M)\n",
    "    \n",
    "    # Now we sum up each column to create a vector of size 25\n",
    "    v = M.sum(axis=0)\n",
    "    \n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    \n",
    "    # I don't understand what the heck this is doing...\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "\n",
    "# Print an example to show what this does...\n",
    "s = xtrain[1973]\n",
    "print (s)\n",
    "print (word_tokenize(s))\n",
    "print (new_tokenize(s))\n",
    "print (sent2vec(s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "568e781a2d92447188d6ac97cc8915f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=5077.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dc458936521d4b7e837964ee75c77f70",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=2536.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54acaab051854ebaacb0d934e10cda01",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=7613.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d862c77f282c49a1b2935b260fffc6f7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3263.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "\n",
    "xtrain_glove = [sent2vec(x) for x in tqdm(xtrain)]\n",
    "xvalid_glove = [sent2vec(x) for x in tqdm(xvalid)]\n",
    "xtrain_full_glove = [sent2vec(x) for x in tqdm(xtrain_full)]\n",
    "xtest_full_glove = [sent2vec(x) for x in tqdm(xtest_full)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xtrain_glove_np = np.stack(xtrain_glove)\n",
    "xvalid_glove_np = np.stack(xvalid_glove)\n",
    "xtrain_full_glove_np = np.stack(xtrain_full_glove)\n",
    "xtest_full_glove_np = np.stack(xtest_full_glove)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Deep Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "from keras import callbacks\n",
    "from keras.utils import np_utils\n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Activation, Dropout\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "\n",
    "# scale the data before any neural net:\n",
    "scl = preprocessing.StandardScaler()\n",
    "\n",
    "xtrain_glove_scl = scl.fit_transform(xtrain_glove_np)\n",
    "xvalid_glove_scl = scl.transform(xvalid_glove_np)\n",
    "xtrain_glove_full_scl = scl.fit_transform(xtrain_full_glove_np)\n",
    "xtest_glove_full_scl = scl.transform(xtest_full_glove_np)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Source: https://datascience.stackexchange.com/questions/45165/how-to-get-accuracy-f1-precision-and-recall-for-a-keras-model\n",
    "from keras import backend as K\n",
    "\n",
    "def recall_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    return recall\n",
    "\n",
    "def precision_m(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    return precision\n",
    "\n",
    "def f1_m(y_true, y_pred):\n",
    "    precision = precision_m(y_true, y_pred)\n",
    "    recall = recall_m(y_true, y_pred)\n",
    "    return 2*((precision*recall)/(precision+recall+K.epsilon()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_size = xtrain_glove_scl.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we need to binarize the labels for the neural net\n",
    "ytrain_enc = np_utils.to_categorical(ytrain)\n",
    "yvalid_enc = np_utils.to_categorical(yvalid)\n",
    "ytrain_full_enc = np_utils.to_categorical(ytrain_full)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create a simple 3 layer sequential neural net\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Dense(300, input_dim=input_size, activation='relu'))\n",
    "model.add(Dropout(0.3))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(300, activation='relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(BatchNormalization())\n",
    "\n",
    "model.add(Dense(2))\n",
    "model.add(Activation('softmax'))\n",
    "\n",
    "# compile the model\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['acc',f1_m,precision_m, recall_m])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.1 Smaller Set with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/500\n",
      "80/80 [==============================] - 0s 4ms/step - loss: 0.6385 - acc: 0.7246 - f1_m: 0.7254 - precision_m: 0.7254 - recall_m: 0.7254 - val_loss: 0.4784 - val_acc: 0.8052 - val_f1_m: 0.8059 - val_precision_m: 0.8059 - val_recall_m: 0.8059\n",
      "Epoch 2/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.5075 - acc: 0.7755 - f1_m: 0.7757 - precision_m: 0.7757 - recall_m: 0.7757 - val_loss: 0.4403 - val_acc: 0.8190 - val_f1_m: 0.8193 - val_precision_m: 0.8193 - val_recall_m: 0.8193\n",
      "Epoch 3/500\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4478 - acc: 0.7979 - f1_m: 0.7972 - precision_m: 0.7972 - recall_m: 0.7972 - val_loss: 0.4364 - val_acc: 0.8190 - val_f1_m: 0.8193 - val_precision_m: 0.8193 - val_recall_m: 0.8193\n",
      "Epoch 4/500\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4213 - acc: 0.8188 - f1_m: 0.8183 - precision_m: 0.8183 - recall_m: 0.8183 - val_loss: 0.4382 - val_acc: 0.8233 - val_f1_m: 0.8234 - val_precision_m: 0.8234 - val_recall_m: 0.8234\n",
      "Epoch 5/500\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.4000 - acc: 0.8188 - f1_m: 0.8191 - precision_m: 0.8191 - recall_m: 0.8191 - val_loss: 0.4414 - val_acc: 0.8202 - val_f1_m: 0.8202 - val_precision_m: 0.8202 - val_recall_m: 0.8202\n",
      "Epoch 6/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3819 - acc: 0.8292 - f1_m: 0.8291 - precision_m: 0.8291 - recall_m: 0.8291 - val_loss: 0.4400 - val_acc: 0.8170 - val_f1_m: 0.8171 - val_precision_m: 0.8171 - val_recall_m: 0.8171\n",
      "Epoch 7/500\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.3623 - acc: 0.8452 - f1_m: 0.8441 - precision_m: 0.8441 - recall_m: 0.8441 - val_loss: 0.4464 - val_acc: 0.8226 - val_f1_m: 0.8226 - val_precision_m: 0.8226 - val_recall_m: 0.8226\n",
      "Epoch 8/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3519 - acc: 0.8444 - f1_m: 0.8445 - precision_m: 0.8445 - recall_m: 0.8445 - val_loss: 0.4521 - val_acc: 0.8198 - val_f1_m: 0.8198 - val_precision_m: 0.8198 - val_recall_m: 0.8198\n",
      "Epoch 9/500\n",
      "80/80 [==============================] - 0s 3ms/step - loss: 0.3415 - acc: 0.8535 - f1_m: 0.8543 - precision_m: 0.8543 - recall_m: 0.8543 - val_loss: 0.4628 - val_acc: 0.8257 - val_f1_m: 0.8257 - val_precision_m: 0.8257 - val_recall_m: 0.8257\n",
      "Epoch 10/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3342 - acc: 0.8564 - f1_m: 0.8564 - precision_m: 0.8564 - recall_m: 0.8564 - val_loss: 0.4634 - val_acc: 0.8182 - val_f1_m: 0.8185 - val_precision_m: 0.8185 - val_recall_m: 0.8185\n",
      "Epoch 11/500\n",
      "80/80 [==============================] - 0s 2ms/step - loss: 0.3202 - acc: 0.8651 - f1_m: 0.8658 - precision_m: 0.8658 - recall_m: 0.8658 - val_loss: 0.4773 - val_acc: 0.8186 - val_f1_m: 0.8189 - val_precision_m: 0.8189 - val_recall_m: 0.8189\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1f2c303d588>"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = callbacks.EarlyStopping(monitor='val_f1_m', patience=10)\n",
    "\n",
    "model.fit(xtrain_glove_scl, y=ytrain_enc, batch_size=64, \n",
    "          epochs=500, verbose=1, \n",
    "          validation_data=(xvalid_glove_scl, yvalid_enc),\n",
    "          callbacks=[early_stop])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TFIDF Result:\t\t0.779270633397313\n"
     ]
    }
   ],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(f'TFIDF Result:\\t\\t{metrics.f1_score(model.predict(xvalid_glove_scl).argmax(axis=1), yvalid)}')  # Result: 0.7081524360829716"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Full Set with no Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.6192 - acc: 0.7387 - f1_m: 0.7388 - precision_m: 0.7388 - recall_m: 0.7388\n",
      "Epoch 2/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.4925 - acc: 0.7833 - f1_m: 0.7833 - precision_m: 0.7833 - recall_m: 0.7833\n",
      "Epoch 3/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.4342 - acc: 0.8070 - f1_m: 0.8071 - precision_m: 0.8071 - recall_m: 0.8071\n",
      "Epoch 4/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.4152 - acc: 0.8186 - f1_m: 0.8186 - precision_m: 0.8186 - recall_m: 0.8186\n",
      "Epoch 5/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.3992 - acc: 0.8250 - f1_m: 0.8251 - precision_m: 0.8251 - recall_m: 0.8251\n",
      "Epoch 6/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.3851 - acc: 0.8344 - f1_m: 0.8343 - precision_m: 0.8343 - recall_m: 0.8343\n",
      "Epoch 7/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.3725 - acc: 0.8370 - f1_m: 0.8370 - precision_m: 0.8370 - recall_m: 0.8370\n",
      "Epoch 8/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.3692 - acc: 0.8370 - f1_m: 0.8370 - precision_m: 0.8370 - recall_m: 0.8370\n",
      "Epoch 9/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.3548 - acc: 0.8438 - f1_m: 0.8438 - precision_m: 0.8438 - recall_m: 0.8438\n",
      "Epoch 10/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.3486 - acc: 0.8520 - f1_m: 0.8520 - precision_m: 0.8520 - recall_m: 0.8520\n",
      "Epoch 11/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.3312 - acc: 0.8601 - f1_m: 0.8601 - precision_m: 0.8601 - recall_m: 0.8601\n",
      "Epoch 12/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.3220 - acc: 0.8646 - f1_m: 0.8646 - precision_m: 0.8646 - recall_m: 0.8646\n",
      "Epoch 13/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.3104 - acc: 0.8675 - f1_m: 0.8675 - precision_m: 0.8675 - recall_m: 0.8675\n",
      "Epoch 14/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.2992 - acc: 0.8765 - f1_m: 0.8765 - precision_m: 0.8765 - recall_m: 0.8765\n",
      "Epoch 15/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.2863 - acc: 0.8794 - f1_m: 0.8794 - precision_m: 0.8794 - recall_m: 0.8794\n",
      "Epoch 16/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.2754 - acc: 0.8866 - f1_m: 0.8867 - precision_m: 0.8867 - recall_m: 0.8867\n",
      "Epoch 17/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.2666 - acc: 0.8868 - f1_m: 0.8868 - precision_m: 0.8868 - recall_m: 0.8868\n",
      "Epoch 18/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.2638 - acc: 0.8923 - f1_m: 0.8923 - precision_m: 0.8923 - recall_m: 0.8923\n",
      "Epoch 19/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.2598 - acc: 0.8939 - f1_m: 0.8939 - precision_m: 0.8939 - recall_m: 0.8939\n",
      "Epoch 20/20\n",
      "119/119 [==============================] - 0s 2ms/step - loss: 0.2350 - acc: 0.9007 - f1_m: 0.9007 - precision_m: 0.9007 - recall_m: 0.9007\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x29bedaad688>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "early_stop = callbacks.EarlyStopping(monitor='val_f1_m', patience=10)\n",
    "\n",
    "model.fit(xtrain_glove_full_scl, y=ytrain_full_enc, batch_size=64, \n",
    "          epochs=20, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = model.predict(xtest_glove_full_scl).argmax(axis=1)\n",
    "output = pd.DataFrame({'id': test_df.id, 'target': predictions})\n",
    "output.to_csv('my_submission.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
